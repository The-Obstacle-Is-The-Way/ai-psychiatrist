# AI Psychiatrist Configuration (Paper-Optimal)
# Copy to .env and fill in values

# ============== Required ==============
OLLAMA_HOST=127.0.0.1
OLLAMA_PORT=11434

# ============== LLM Backend (Optional) ==============
# Default backend is Ollama. Use HuggingFace only if you need official model weights
# that are not available in the Ollama library (e.g., MedGemma).
# LLM_BACKEND=huggingface
# LLM_HF_DEVICE=auto          # auto/cpu/cuda/mps
# LLM_HF_QUANTIZATION=int4    # optional: int4/int8

# ============== Embedding Backend (Optional) ==============
# Embeddings can run on a different backend than chat via EMBEDDING_BACKEND.
# Default embedding backend is HuggingFace (FP16 precision) for better similarity scores.
# If you don't have HF deps installed, set EMBEDDING_BACKEND=ollama.
# EMBEDDING_BACKEND=huggingface
#
# Embeddings artifact selection (used for few-shot retrieval):
# Default: huggingface_qwen3_8b_paper_train (FP16, higher quality)
# Alternative: paper_reference_embeddings (Ollama Q4_K_M, paper-parity)
#
# To select a different artifact:
#   EMBEDDING_EMBEDDINGS_FILE=paper_reference_embeddings
#
# To override with full path:
#   DATA_EMBEDDINGS_PATH=data/embeddings/huggingface_qwen3_8b_paper_train.npz

# ============== LLM Models (Paper-Optimal) ==============
# All agents use Gemma 3 27B (Paper Section 2.2)
# NOTE: MedGemma (Appendix F) has better item-level MAE but makes fewer predictions overall
#
# Production-recommended: gemma3:27b-it-qat (QAT-quantized, faster inference)
# Paper-baseline tag: gemma3:27b (GGUF Q4_K_M; closer naming parity, not BF16)
MODEL_EMBEDDING_MODEL=qwen3-embedding:8b
MODEL_JUDGE_MODEL=gemma3:27b-it-qat
MODEL_META_REVIEW_MODEL=gemma3:27b-it-qat
MODEL_QUALITATIVE_MODEL=gemma3:27b-it-qat
MODEL_QUANTITATIVE_MODEL=gemma3:27b-it-qat

# Alternative (Appendix F - MedGemma; requires HuggingFace backend for official weights):
# LLM_BACKEND=huggingface
# MODEL_QUANTITATIVE_MODEL=medgemma:27b

# ============== Embedding/Few-Shot (Paper Appendix D) ==============
# Paper optimal hyperparameters:
EMBEDDING_CHUNK_SIZE=8
EMBEDDING_CHUNK_STEP=2
EMBEDDING_DIMENSION=4096
EMBEDDING_TOP_K_REFERENCES=2

# ============== Retrieval Quality (Specs 32-37) ==============
# See FEATURES.md for full documentation.

# Spec 32: Retrieval audit logging (logs similarity scores and chunks)
EMBEDDING_ENABLE_RETRIEVAL_AUDIT=true

# Spec 37: Batch query embedding + timeout (fixes query embedding timeouts)
# Default: enabled (True) and 300s timeout.
# EMBEDDING_ENABLE_BATCH_QUERY_EMBEDDING=true
# EMBEDDING_QUERY_EMBED_TIMEOUT_SECONDS=300

# Spec 33: Retrieval quality guardrails
# Drop references below minimum similarity (0.0 = disabled)
EMBEDDING_MIN_REFERENCE_SIMILARITY=0.3
# Limit total context characters per PHQ-8 item (0 = disabled)
EMBEDDING_MAX_REFERENCE_CHARS_PER_ITEM=500

# Spec 34: Item-tag filtering (requires .tags.json sidecar from generate_embeddings --write-item-tags)
EMBEDDING_ENABLE_ITEM_TAG_FILTER=true

# Spec 35: Chunk-level scoring (NOT paper-parity - use for ablation studies)
# Default: "participant" (paper-parity)
# Alternative: "chunk" (experimental - requires running score_reference_chunks.py first)
#
# STEP 1: Generate chunk scores (one-time preprocessing):
#   python scripts/score_reference_chunks.py \
#     --embeddings-file <your_embeddings_file> \
#     --scorer-backend ollama \
#     --scorer-model gemma3:27b-it-qat \
#     --allow-same-model
#
# STEP 2: Enable chunk-level scoring (AFTER running step 1):
# EMBEDDING_REFERENCE_SCORE_SOURCE=chunk
#
# Optional unsafe override (NOT recommended): allow loading chunk scores when the scorer prompt changes.
# EMBEDDING_ALLOW_CHUNK_SCORES_PROMPT_HASH_MISMATCH=false

# Spec 36: CRAG-style runtime reference validation (adds LLM calls per reference)
# EMBEDDING_ENABLE_REFERENCE_VALIDATION=true
# Optional: override validation model (default: MODEL_JUDGE_MODEL)
# EMBEDDING_VALIDATION_MODEL=gemma3:27b-it-qat
# Optional: max accepted refs per item after validation
# EMBEDDING_VALIDATION_MAX_REFS_PER_ITEM=2

# LLM Model Configuration
# ------------------------------------------------------------------------------
# Defaults for clinical reproducibility:
# - All agents: temp=0.0 (Med-PaLM, medRxiv 2025)
# - top_k/top_p: REMOVED (irrelevant at temp=0, best practice is temp only)

MODEL_TEMPERATURE=0.0
# top_k/top_p: NOT SET (irrelevant at temp=0, best practice is temp only)

# ============== Feedback Loop (Paper Section 2.3.1) ==============
FEEDBACK_ENABLED=true
FEEDBACK_MAX_ITERATIONS=10
# Paper: "score below four" triggers refinement (threshold=3 means <4)
FEEDBACK_SCORE_THRESHOLD=3

# ============== Quantitative (SPEC-003) ==============
# Keyword backfill is OFF by default for paper parity (~50% coverage).
# Enable for higher coverage (~74%) at the cost of deviating from paper methodology.
QUANTITATIVE_ENABLE_KEYWORD_BACKFILL=false
QUANTITATIVE_TRACK_NA_REASONS=true
# Max keyword-matched sentences per PHQ-8 item when backfill is enabled.
QUANTITATIVE_KEYWORD_BACKFILL_CAP=3

# ============== Server ==============
API_HOST=0.0.0.0
API_PORT=8000
OLLAMA_TIMEOUT_SECONDS=600  # 10 min default; use 3600 for slow GPUs

# ============== Logging ==============
LOG_FORMAT=json  # json or console
LOG_LEVEL=INFO
# LOG_FORCE_COLORS=true   # force ANSI colors even when piped
# LOG_FORCE_COLORS=false  # force no colors even in a TTY
#
# Standard convention:
# NO_COLOR=1              # disable ANSI colors even in a TTY (https://no-color.org/)

# ============== Pydantic AI ==============
# Pydantic AI is ENABLED by default for structured output validation + automatic retries.
# Legacy parsing fallbacks are disabled (fail-fast research behavior).
#
# Disabling is not supported (agents require Pydantic AI):
# PYDANTIC_AI_ENABLED=false
#
# Retry count (0 disables retries):
# PYDANTIC_AI_RETRIES=3
#
# Timeout override for Pydantic AI calls (unset = library default, currently 600s):
# PYDANTIC_AI_TIMEOUT_SECONDS=3600
