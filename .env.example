# AI Psychiatrist Configuration (Validated Baseline)
# Copy to .env and fill in values

# ============== Required ==============
OLLAMA_HOST=127.0.0.1
OLLAMA_PORT=11434

# ============== Data ==============
# Transcript root. Prefer deterministic participant-only preprocessing for retrieval/embeddings.
DATA_TRANSCRIPTS_DIR=data/transcripts_participant_only
# Raw transcripts (includes Ellie prompts; legacy baseline, not recommended):
# DATA_TRANSCRIPTS_DIR=data/transcripts

# ============== LLM Backend (Optional) ==============
# Default backend is Ollama. Use HuggingFace only if you need official model weights
# that are not available in the Ollama library (e.g., MedGemma).
# LLM_BACKEND=huggingface
LLM_BACKEND=ollama
# LLM_HF_DEVICE=auto          # auto/cpu/cuda/mps
# LLM_HF_QUANTIZATION=int4    # optional: int4/int8

# ============== Embedding Backend ==============
# IMPORTANT: HuggingFace (FP16) produces better similarity scores than Ollama (Q4_K_M).
# Use Ollama only if you don't have HuggingFace deps installed.
#
# Backend + artifact MUST match:
#   huggingface → huggingface_qwen3_8b_paper_train_participant_only
#   ollama      → ollama_qwen3_8b_paper_train_participant_only
#
EMBEDDING_BACKEND=huggingface
EMBEDDING_EMBEDDINGS_FILE=huggingface_qwen3_8b_paper_train_participant_only
#
# Fallback (if no HuggingFace deps):
# EMBEDDING_BACKEND=ollama
# EMBEDDING_EMBEDDINGS_FILE=ollama_qwen3_8b_paper_train_participant_only

# ============== LLM Models (Validated Baseline) ==============
# All agents use Gemma 3 27B (Paper Section 2.2)
# NOTE: MedGemma (Appendix F) has better item-level MAE but makes fewer predictions overall
#
# Production-recommended: gemma3:27b-it-qat (QAT-quantized, faster inference)
# Paper-baseline tag: gemma3:27b (GGUF Q4_K_M; closer naming parity, not BF16)
MODEL_EMBEDDING_MODEL=qwen3-embedding:8b
MODEL_JUDGE_MODEL=gemma3:27b-it-qat
MODEL_META_REVIEW_MODEL=gemma3:27b-it-qat
MODEL_QUALITATIVE_MODEL=gemma3:27b-it-qat
MODEL_QUANTITATIVE_MODEL=gemma3:27b-it-qat

# Alternative (Appendix F - MedGemma; requires HuggingFace backend for official weights):
# LLM_BACKEND=huggingface
# MODEL_QUANTITATIVE_MODEL=medgemma:27b

# ============== Embedding/Few-Shot (Baseline Hyperparameters - Paper Appendix D) ==============
EMBEDDING_CHUNK_SIZE=8
EMBEDDING_CHUNK_STEP=2
EMBEDDING_DIMENSION=4096
EMBEDDING_ALLOW_INSUFFICIENT_DIMENSION_EMBEDDINGS=false
EMBEDDING_TOP_K_REFERENCES=2

# ============== Retrieval Quality (Specs 32-37) ==============
# See FEATURES.md for full documentation.

# Spec 32: Retrieval audit logging (logs similarity scores and chunks)
EMBEDDING_ENABLE_RETRIEVAL_AUDIT=true

# Spec 37: Batch query embedding + timeout (fixes query embedding timeouts)
# Default: enabled (True) and 300s timeout.
# EMBEDDING_ENABLE_BATCH_QUERY_EMBEDDING=true
# EMBEDDING_QUERY_EMBED_TIMEOUT_SECONDS=300

# Spec 33: Retrieval quality guardrails
# Limit total context characters per PHQ-8 item (0 = disabled)
EMBEDDING_MAX_REFERENCE_CHARS_PER_ITEM=500
# Drop references below minimum similarity (0.0 = disabled)
EMBEDDING_MIN_REFERENCE_SIMILARITY=0.3

# Spec 34: Item-tag filtering (requires .tags.json sidecar from generate_embeddings --write-item-tags)
EMBEDDING_ENABLE_ITEM_TAG_FILTER=true

# Spec 35: Chunk-level scoring (recommended; requires preprocessing)
# Default: "participant" (legacy baseline; assigns participant-level scores to chunks)
# Recommended: "chunk" (requires running score_reference_chunks.py first)
#
# STEP 1: Generate chunk scores (one-time preprocessing):
#   python scripts/score_reference_chunks.py \
#     --embeddings-file <your_embeddings_file> \
#     --scorer-backend ollama \
#     --scorer-model gemma3:27b-it-qat \
#     --allow-same-model
#
# STEP 2: Enable chunk-level scoring (AFTER running step 1):
EMBEDDING_REFERENCE_SCORE_SOURCE=chunk
#
# Optional unsafe override (NOT recommended): allow loading chunk scores when the scorer prompt changes.
# EMBEDDING_ALLOW_CHUNK_SCORES_PROMPT_HASH_MISMATCH=false

# Spec 36: CRAG-style runtime reference validation (adds LLM calls per reference)
# EMBEDDING_ENABLE_REFERENCE_VALIDATION=true
# Optional: override validation model (default: MODEL_JUDGE_MODEL)
# EMBEDDING_VALIDATION_MODEL=gemma3:27b-it-qat
# Optional: max accepted refs per item after validation
# EMBEDDING_VALIDATION_MAX_REFS_PER_ITEM=2

# LLM Model Configuration
# ------------------------------------------------------------------------------
# Defaults for clinical reproducibility:
# - All agents: temp=0.0 (Med-PaLM, medRxiv 2025)
# - top_k/top_p: REMOVED (irrelevant at temp=0, best practice is temp only)

MODEL_TEMPERATURE=0.0
# top_k/top_p: NOT SET (irrelevant at temp=0, best practice is temp only)

# ============== Feedback Loop (Paper Section 2.3.1) ==============
FEEDBACK_ENABLED=true
FEEDBACK_MAX_ITERATIONS=10
# Paper: "score below four" triggers refinement (threshold=3 means <4)
FEEDBACK_SCORE_THRESHOLD=3

# ============== Quantitative ==============
QUANTITATIVE_TRACK_NA_REASONS=true
#
# Spec 053: Evidence grounding (prevents hallucinated quotes from contaminating retrieval)
QUANTITATIVE_EVIDENCE_QUOTE_VALIDATION_ENABLED=true
QUANTITATIVE_EVIDENCE_QUOTE_VALIDATION_MODE="substring"  # substring (default) or fuzzy (requires rapidfuzz)
QUANTITATIVE_EVIDENCE_QUOTE_FUZZY_THRESHOLD=0.85
QUANTITATIVE_EVIDENCE_QUOTE_FAIL_ON_ALL_REJECTED=true
QUANTITATIVE_EVIDENCE_QUOTE_LOG_REJECTIONS=true

# ============== Consistency Confidence (Spec 050) ==============
# Optional multi-sample scoring to derive agreement-based confidence signals.
CONSISTENCY_ENABLED=false
CONSISTENCY_N_SAMPLES=5
CONSISTENCY_TEMPERATURE=0.3

# ============== Server ==============
API_HOST=0.0.0.0
API_PORT=8000
OLLAMA_TIMEOUT_SECONDS=600  # 10 min default; use 3600 for slow GPUs

# ============== Logging ==============
LOG_FORMAT=console  # json or console
LOG_LEVEL=INFO
# LOG_FORCE_COLORS=true   # force ANSI colors even when piped
# LOG_FORCE_COLORS=false  # force no colors even in a TTY
#
# Standard convention:
# NO_COLOR=1              # disable ANSI colors even in a TTY (https://no-color.org/)

# ============== Pydantic AI ==============
# Pydantic AI is ENABLED by default for structured output validation + automatic retries.
# Legacy parsing fallbacks are disabled (fail-fast research behavior).
#
# Disabling is not supported (agents require Pydantic AI):
# PYDANTIC_AI_ENABLED=false
#
# Retry count (0 disables retries):
# PYDANTIC_AI_RETRIES=3
#
# Timeout override for Pydantic AI calls (unset = library default, currently 600s):
# PYDANTIC_AI_TIMEOUT_SECONDS=3600
PYDANTIC_AI_ENABLED=true
PYDANTIC_AI_RETRIES=3
