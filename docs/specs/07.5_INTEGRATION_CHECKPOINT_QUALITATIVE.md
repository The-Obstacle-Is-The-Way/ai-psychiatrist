# Spec 07.5: Post-Qualitative Path Integration Checkpoint

## Overview

**Checkpoint Location**: After Spec 07 (Judge Agent), before Spec 08 (Embedding Service)

**Purpose**: Validate the qualitative assessment path is complete and working end-to-end before building the quantitative path.

**Duration**: This is a **MANDATORY PAUSE** for quality review.

## Checkpoint Rationale

At this point, we have completed:
- **Specs 01-04A**: Foundation & Infrastructure (verified at Checkpoint 04.5)
- **Spec 05**: Transcript Service (loading and parsing DAIC-WOZ transcripts)
- **Spec 06**: Qualitative Agent (PHQ-8 symptom narrative analysis)
- **Spec 07**: Judge Agent (self-refinement feedback loop)

This represents the **first complete vertical slice**: a working qualitative assessment pipeline that can process transcripts and produce evaluated assessments.

## Vertical Integration Test

### End-to-End Flow

```
Transcript CSV → TranscriptService → QualitativeAgent → JudgeAgent → Refined Assessment
```

**Test Command**:
```bash
# Integration test for qualitative path
pytest tests/integration/test_qualitative_pipeline.py -v

# Or manual verification
python -m ai_psychiatrist.cli assess --participant-id 300 --mode qualitative
```

### Expected Outputs

1. **Transcript Loading**:
   - Loads `data/transcripts/300_P/300_TRANSCRIPT.csv`
   - Parses speaker turns (Ellie vs Participant)
   - Produces structured `Transcript` entity

2. **Qualitative Assessment**:
   - Produces narrative for each PHQ-8 symptom
   - Confidence scores for each symptom
   - Evidence citations from transcript

3. **Judge Feedback**:
   - Scores on 4 metrics (completeness, relevance, accuracy, insight)
   - Improvement suggestions
   - Feedback loop terminates when scores ≥ threshold (paper: ≤2 mistakes per metric)

## Bug Hunt Protocol

### P0: Critical (Block all forward progress)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| Qualitative agent produces empty output | Integration test | No symptoms analyzed |
| Judge scores always fail threshold | Integration test | Infinite feedback loop |
| Transcript loading crashes on real data | Test with actual DAIC-WOZ | Parse errors |
| LLM responses unparseable | Integration test | JSON extraction failure |
| Feedback loop exceeds max iterations | Integration test | >10 iterations (paper limit) |

### P1: High (Fix before next checkpoint)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| Missing PHQ-8 symptoms in output | Output validation | Only 6 of 8 symptoms |
| Evidence citations don't match transcript | Manual review | Hallucinated quotes |
| Judge metrics inconsistent across runs | Multiple runs | High variance |
| Silent fallbacks on LLM errors | Log review | No error logging |
| Prompt templates have hardcoded values | Grep for literals | `model = "gemma3:27b"` |

### P2: Medium (Track for later)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| Prompt quality could improve | Human review | Verbose or unclear prompts |
| Response parsing too fragile | Edge case testing | Unusual LLM output format |
| Logging missing key context | Log review | Can't reconstruct failures |

## Quality Gates

### Gate 1: Functional Parity with Paper

```python
# Must match paper Section 2.3.1 behavior:
# - Qualitative assessment for all 8 PHQ-8 symptoms
# - Judge agent with 4 evaluation metrics
# - Feedback loop with ≤2 threshold, max 10 iterations
```

**Verification**:
```bash
# Check symptom coverage
pytest tests/unit/agents/test_qualitative_agent.py::test_all_phq8_symptoms

# Check judge metrics
pytest tests/unit/agents/test_judge_agent.py::test_judge_metrics_match_paper
```

### Gate 2: LLM Abstraction Integrity

- [ ] No direct `requests.post()` calls to Ollama in agent code
- [ ] All LLM calls go through `LLMClient` protocol
- [ ] Agents receive `LLMClient` via dependency injection
- [ ] Mock tests don't bypass the protocol

### Gate 3: Error Handling

```bash
# Test error scenarios
pytest tests/integration/test_qualitative_errors.py -v

# Scenarios:
# - Ollama server unreachable
# - Malformed LLM response
# - Empty transcript
# - Timeout on LLM call
```

**Acceptance**: All error scenarios handled gracefully with appropriate logging.

### Gate 4: Performance Baseline

```bash
# Measure qualitative pipeline latency
time python -m ai_psychiatrist.cli assess --participant-id 300 --mode qualitative

# Paper reference: Full pipeline ~1 minute on M3 Pro
# Qualitative path alone should be: < 30 seconds
```

## Data Integration Validation

### Transcript Service Checks

- [ ] Can load all 189 participants from `data/transcripts/`
- [ ] Handles both `Participant` and `Ellie` speaker labels
- [ ] Correctly parses tab-separated CSV format
- [ ] Handles transcripts with unusual formatting

### Test with Multiple Participants

```bash
# Sample across dataset
for pid in 300 303 310 400 492; do
    python -m ai_psychiatrist.cli assess --participant-id $pid --mode qualitative --dry-run
done
```

## Agent Smell Detection

### Anti-Patterns to Check

| Smell | Detection | Fix |
|-------|-----------|-----|
| God Agent | Agent does parsing + LLM + validation | Split into services |
| Hardcoded Prompts | Prompts in agent methods | Move to templates |
| Stateful Agents | Instance variables between calls | Make agents stateless |
| Tight Coupling | Agent knows about Ollama specifics | Use protocol only |
| Magic Numbers | `threshold = 2` inline | Define as config |

### Grep Commands

```bash
# Check for hardcoded thresholds
grep -rE "threshold\s*=\s*[0-9]" src/ai_psychiatrist/agents/

# Check for direct HTTP calls
grep -r "requests\." src/ai_psychiatrist/agents/
grep -r "httpx\." src/ai_psychiatrist/agents/

# Check for hardcoded model names
grep -rE "gemma|llama|mistral" src/ai_psychiatrist/agents/
```

## Prompt Template Review

### Qualitative Agent Prompts

- [ ] System prompt matches paper methodology
- [ ] User prompt includes full transcript context
- [ ] Output format instructions are clear
- [ ] Prompt versioned/tracked for reproducibility

### Judge Agent Prompts

- [ ] Evaluation rubric matches paper Appendix B
- [ ] 4 metrics clearly defined
- [ ] Scoring scale (0-5 mistakes) is clear
- [ ] Improvement suggestions format specified

## Technical Debt Inventory Update

### New Debt from Specs 05-07

| Item | Location | Severity | Notes |
|------|----------|----------|-------|
| [Document any new debt] | | | |

### Resolved Debt

| Item | Resolution | Spec |
|------|------------|------|
| [Document resolved items] | | |

### Debt Remaining from Checkpoint 04.5

- [ ] Review and update status

## Review Checklist

### Code Quality (Specs 05-07)

- [ ] TranscriptService has zero linting errors
- [ ] QualitativeAgent has zero linting errors
- [ ] JudgeAgent has zero linting errors
- [ ] All public functions have docstrings
- [ ] No `print()` statements
- [ ] No bare `except:` clauses

### Architecture

- [ ] Agents don't import from `infrastructure.llm.ollama` directly
- [ ] Agents depend on `LLMClient` protocol only
- [ ] TranscriptService is independent of agent implementation
- [ ] Domain entities are immutable

### Testing

- [ ] Unit tests for TranscriptService
- [ ] Unit tests for QualitativeAgent (mocked LLM)
- [ ] Unit tests for JudgeAgent (mocked LLM)
- [ ] Integration test for qualitative pipeline
- [ ] No mock abuse in tests

### Paper Parity

- [ ] PHQ-8 symptoms match paper exactly
- [ ] Judge metrics match Appendix B
- [ ] Feedback loop threshold matches paper (≤2)
- [ ] Max iterations match paper (10)

## Exit Criteria

**This checkpoint is COMPLETE when:**

1. [ ] All P0 issues resolved
2. [ ] All P1 issues either resolved or documented with plan
3. [ ] Qualitative pipeline runs end-to-end on real data
4. [ ] CI/CD pipeline green
5. [ ] Test coverage ≥ 80% for Specs 05-07 code
6. [ ] Senior review approved
7. [ ] Technical debt inventory updated

## Next Steps

After passing this checkpoint:
1. Proceed to **Spec 08: Embedding Service**
2. Begin quantitative path implementation
3. Next checkpoint: **Spec 09.5** (after quantitative path)

## Reference Commands

```bash
# Full quality check
make check

# Integration test
pytest tests/integration/test_qualitative_pipeline.py -v

# Coverage for specs 05-07
pytest --cov=src/ai_psychiatrist/services --cov=src/ai_psychiatrist/agents --cov-report=term-missing

# Manual pipeline test
python -m ai_psychiatrist.cli assess --participant-id 300 --mode qualitative

# Agent smell detection
grep -rE "threshold\s*=\s*[0-9]" src/ai_psychiatrist/agents/
grep -r "requests\." src/ai_psychiatrist/
grep -rE "gemma|llama|mistral" src/ai_psychiatrist/agents/
```
