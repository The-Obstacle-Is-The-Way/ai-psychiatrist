# Spec 11.5: Full Pipeline Integration Checkpoint

## Overview

**Checkpoint Location**: After Spec 11 (Full Pipeline API), before Spec 12 (Observability)

**Purpose**: Validate the complete multi-agent system works end-to-end before adding observability polish.

**Duration**: This is a **MANDATORY PAUSE** and the **most critical checkpoint**.

## Checkpoint Rationale

At this point, we have completed:
- **Specs 01-04A**: Foundation & Infrastructure
- **Specs 05-07**: Qualitative Path (verified at Checkpoint 07.5)
- **Specs 08-09**: Quantitative Path (verified at Checkpoint 09.5)
- **Spec 10**: Meta-Review Agent (severity integration)
- **Spec 11**: Full Pipeline API (complete assessment endpoint)

This is the **functional complete** state. The system can:
1. Accept a transcript
2. Run qualitative assessment with feedback loop
3. Run quantitative scoring with few-shot prompting
4. Integrate via meta-review into final severity prediction

**Why pause here?** Observability (Spec 12) is polish. If the pipeline doesn't work, no amount of logging will save it. This checkpoint ensures **functional correctness before beautification**.

## Full System Integration Test

### Complete Flow

```
API Request (participant_id)
    │
    ▼
TranscriptService
    │
    ├───────────────────────────────────────┐
    ▼                                       ▼
QualitativeAgent ──→ JudgeAgent      EmbeddingService
    │               (feedback loop)         │
    ▼                                       ▼
Qualitative Assessment              QuantitativeAgent
    │                                       │
    └───────────────┬───────────────────────┘
                    ▼
              MetaReviewAgent
                    │
                    ▼
           Final Assessment
    (severity, diagnosis, confidence)
                    │
                    ▼
              API Response
```

**Test Commands**:

```bash
# Full E2E integration test
pytest tests/e2e/test_full_pipeline.py -v

# Manual verification via CLI
python -m ai_psychiatrist.cli assess --participant-id 300

# API verification
curl -X POST http://localhost:8000/assess \
  -H "Content-Type: application/json" \
  -d '{"participant_id": 300}'
```

### Expected Final Output

```json
{
  "participant_id": 300,
  "qualitative_assessment": {
    "symptoms": [...],  // 8 PHQ-8 symptoms with narratives
    "judge_scores": [...],  // Final judge metrics
    "refinement_iterations": 2
  },
  "quantitative_assessment": {
    "scores": [1, 2, 0, 1, 0, 2, 1, 0],  // 8 PHQ-8 scores (0-3)
    "total": 7,
    "evidence": [...]
  },
  "meta_review": {
    "severity": "mild",  // none/mild/moderate/moderately_severe/severe
    "confidence": 0.85,
    "reasoning": "..."
  },
  "processing_time_ms": 45000
}
```

## Bug Hunt Protocol

### P0: Critical (Block all forward progress)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| Pipeline produces no output | E2E test | Empty response |
| Meta-review crashes on valid input | Integration test | Exception on combine |
| Severity prediction always same value | Aggregate test | All "moderate" |
| API endpoint not responding | Health check | 500 errors |
| Results not matching paper metrics | Evaluation script | MAE > 1.0 |

### P1: High (Fix before next checkpoint)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| Inconsistent qual/quant → meta-review | Cross-validation | Qual severe, quant minimal, meta moderate |
| Pipeline timeout on some transcripts | Performance test | >5 min for long transcripts |
| Memory leak during batch processing | Resource monitoring | OOM after 50 transcripts |
| Race conditions in parallel paths | Concurrent tests | Intermittent failures |
| Error propagation unclear | Log analysis | Silent failures mid-pipeline |

### P2: Medium (Track for later)

| Issue | Detection Method | Example |
|-------|-----------------|---------|
| API response format inconsistent | Schema validation | Missing fields |
| Error messages not user-friendly | Manual review | Stack traces in response |
| No request ID for tracing | Log review | Can't correlate requests |

## Quality Gates

### Gate 1: Paper Metric Reproduction

**Target Metrics (from paper Tables 1, 2)**:

| Metric | Paper Value | Acceptable Range | Check Command |
|--------|-------------|------------------|---------------|
| Quantitative MAE (few-shot) | 0.619 | 0.55 - 0.70 | `make eval-quant` |
| Quantitative MAE (zero-shot) | 0.796 | 0.70 - 0.90 | Reference |
| Severity Accuracy | 0.766 | 0.70 - 0.85 | `make eval-severity` |
| Severity Balanced Accuracy | 0.555 | 0.50 - 0.65 | `make eval-severity` |
| Judge Mean Score (post-feedback) | ~1.5 | 1.0 - 2.0 | `make eval-judge` |

**Verification Script**:

```bash
# Run full evaluation on test set
python -m ai_psychiatrist.cli evaluate --split test --comprehensive

# Expected output:
# Quantitative MAE: 0.XXX
# Severity Accuracy: 0.XXX
# Severity Balanced Accuracy: 0.XXX
# Processing time (avg): XXXms
```

### Gate 2: API Completeness

- [ ] `POST /assess` - Full pipeline assessment
- [ ] `POST /assess/qualitative` - Qualitative only
- [ ] `POST /assess/quantitative` - Quantitative only
- [ ] `GET /health` - Health check
- [ ] `GET /ready` - Readiness check
- [ ] OpenAPI documentation at `/docs`

### Gate 3: Error Handling Completeness

Test each error scenario:

```bash
# Invalid participant ID
curl -X POST http://localhost:8000/assess -d '{"participant_id": 99999}'
# Expected: 404 with clear message

# Ollama unavailable
# (stop Ollama, then)
curl -X POST http://localhost:8000/assess -d '{"participant_id": 300}'
# Expected: 503 with retry guidance

# Malformed request
curl -X POST http://localhost:8000/assess -d 'not json'
# Expected: 400 with validation error

# Timeout scenario
# (use very long transcript)
# Expected: 504 with partial result or clear timeout message
```

### Gate 4: Performance Baseline

| Metric | Paper Reference | Target | Measured |
|--------|-----------------|--------|----------|
| Full pipeline (single) | ~1 min on M3 Pro | < 2 min | |
| Qualitative only | ~30 sec | < 45 sec | |
| Quantitative only | ~20 sec | < 30 sec | |
| Embedding retrieval | ~100ms | < 200ms | |
| API response (cached) | N/A | < 500ms | |

**Measurement**:

```bash
# Benchmark single assessment
time python -m ai_psychiatrist.cli assess --participant-id 300

# Benchmark batch (10 participants)
time python -m ai_psychiatrist.cli batch-assess --ids 300,303,310,315,320,325,330,335,340,345
```

## Multi-Agent Integration Validation

### Agent Coordination Checks

| Check | Method | Pass Criteria |
|-------|--------|---------------|
| Agents don't share state | Code review | No global variables |
| Agents can run in parallel | Concurrent test | No race conditions |
| Failure in one agent isolates | Fault injection | Other paths continue |
| Retry logic works | Simulate flaky Ollama | Eventually succeeds |

### Data Flow Integrity

```
Transcript → QualAgent → JudgeAgent → MetaReview
                         ↑ feedback loop (max 10 iter)

Transcript → EmbeddingService → QuantAgent → MetaReview
```

- [ ] Qualitative output schema matches meta-review input
- [ ] Quantitative output schema matches meta-review input
- [ ] Meta-review can handle missing inputs gracefully
- [ ] All intermediate results are logged for debugging

### Paper Figure Reproduction

| Figure | What it shows | Verification |
|--------|---------------|--------------|
| Figure 1 | Multi-agent architecture | Manual review of flow |
| Figure 4 | PHQ-8 confusion matrices | Generate from test output |
| Figure 6 | Severity confusion matrices | Generate from test output |
| Table 1 | Severity metrics | Compute from test output |

## Legacy Code Status Check

At this checkpoint, we should have **replacements** for all legacy code:

| Legacy Location | Replacement Location | Status |
|-----------------|---------------------|--------|
| `/agents/qualitative_assessor_f.py` | `src/ai_psychiatrist/agents/qualitative.py` | |
| `/agents/qualitative_assessor_z.py` | (config toggle for few/zero-shot) | |
| `/agents/qualitive_evaluator.py` | `src/ai_psychiatrist/agents/judge.py` | |
| `/agents/quantitative_assessor_f.py` | `src/ai_psychiatrist/agents/quantitative.py` | |
| `/agents/meta_reviewer.py` | `src/ai_psychiatrist/agents/meta_review.py` | |
| `/agents/interview_simulator.py` | `src/ai_psychiatrist/services/transcript.py` | |
| `/server.py` | `src/ai_psychiatrist/api/main.py` | |

**If any legacy code is still being used, this checkpoint FAILS.**

## Technical Debt Inventory Update

### Debt That Must Be Resolved Before This Checkpoint

| Item | Status | Notes |
|------|--------|-------|
| No production imports from tests | | Verified? |
| No hardcoded Ollama nodes | | All via config? |
| No direct requests.post() in agents | | Using protocol? |
| All type: ignore justified | | Comments present? |

### Acceptable Debt at This Checkpoint

- Legacy directories still exist (removed in 12.5)
- Observability incomplete (added in Spec 12)
- Documentation gaps (polish later)

### Debt to Document for 12.5

| Item | Location | Severity |
|------|----------|----------|
| Legacy `/agents/` to delete | `/agents/` | P2 |
| Legacy `/server.py` to delete | `/server.py` | P2 |
| Legacy assessment scripts | `/qualitative_assessment/`, etc. | P2 |
| SLURM scripts need update | `/slurm/` | P3 |

## Review Checklist

### Code Quality (Specs 10-11)

- [ ] MetaReviewAgent has zero linting errors
- [ ] API routes have zero linting errors
- [ ] All public functions have docstrings
- [ ] No `print()` statements
- [ ] No bare `except:` clauses

### Architecture

- [ ] API layer only calls services/agents
- [ ] No business logic in API routes
- [ ] Clear request/response models
- [ ] Dependency injection throughout

### Testing

- [ ] Unit tests for MetaReviewAgent
- [ ] Integration tests for full pipeline
- [ ] E2E tests for API endpoints
- [ ] Load test for concurrent requests

### Paper Parity

- [ ] Severity levels match paper (5 levels)
- [ ] Meta-review reasoning matches paper methodology
- [ ] Final output format matches expected structure

## Exit Criteria

**This checkpoint is COMPLETE when:**

1. [ ] All P0 issues resolved
2. [ ] All P1 issues either resolved or documented
3. [ ] Full pipeline runs end-to-end on test set
4. [ ] Paper metrics reproduced within acceptable range
5. [ ] API endpoints all functional
6. [ ] CI/CD pipeline green
7. [ ] Test coverage ≥ 80% for Specs 10-11
8. [ ] Senior review approved
9. [ ] Legacy code no longer imported (though may still exist)

## Next Steps

After passing this checkpoint:
1. Proceed to **Spec 12: Observability**
2. Add logging, metrics, tracing
3. Final checkpoint: **Spec 12.5** (cruft cleanup)

## Reference Commands

```bash
# Full quality check
make check

# E2E test
pytest tests/e2e/test_full_pipeline.py -v

# Full evaluation
python -m ai_psychiatrist.cli evaluate --split test --comprehensive

# Performance benchmark
time python -m ai_psychiatrist.cli assess --participant-id 300

# API test
curl -X POST http://localhost:8000/assess -d '{"participant_id": 300}'

# Legacy code check
grep -r "from agents\." src/
grep -r "import agents\." src/
```
