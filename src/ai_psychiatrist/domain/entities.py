"""Core domain entities for AI Psychiatrist.

Entities are mutable objects with identity (UUID). They represent
the core business concepts and contain business logic for the
depression assessment system.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import UTC, datetime
from typing import TYPE_CHECKING
from uuid import UUID, uuid4

from ai_psychiatrist.domain.enums import (
    AssessmentMode,
    EvaluationMetric,
    PHQ8Item,
    SeverityLevel,
)

if TYPE_CHECKING:
    from collections.abc import Mapping

    from ai_psychiatrist.domain.value_objects import (
        EvaluationScore,
        ItemAssessment,
    )


@dataclass
class Transcript:
    """An interview transcript with metadata.

    Represents the raw interview data from the DAIC-WOZ dataset
    or similar clinical interview sources.
    """

    participant_id: int
    """Unique identifier for the interview participant."""

    text: str
    """The full text content of the interview transcript."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the transcript was created/loaded."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this transcript instance."""

    def __post_init__(self) -> None:
        """Validate transcript data.

        Raises:
            ValueError: If text is empty or participant_id is non-positive.
        """
        if not self.text.strip():
            raise ValueError("Transcript text cannot be empty")
        if self.participant_id <= 0:
            raise ValueError("Participant ID must be positive")

    @property
    def word_count(self) -> int:
        """Count words in the transcript.

        Returns:
            Number of whitespace-separated tokens.
        """
        return len(self.text.split())

    @property
    def line_count(self) -> int:
        """Count lines in the transcript.

        Returns:
            Number of lines (after stripping leading/trailing whitespace).
        """
        return len(self.text.strip().splitlines())


@dataclass
class PHQ8Assessment:
    """Complete PHQ-8 assessment with all 8 items.

    Contains the quantitative assessment results for all 8 PHQ-8
    items, including scores, evidence, and reasoning.
    """

    items: Mapping[PHQ8Item, ItemAssessment]
    """Assessment results for each PHQ-8 item."""

    mode: AssessmentMode
    """Whether zero-shot or few-shot prompting was used."""

    participant_id: int
    """Identifier of the assessed participant."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the assessment was created."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this assessment."""

    def __post_init__(self) -> None:
        """Validate all 8 items are present.

        Raises:
            ValueError: If any PHQ-8 items are missing.
        """
        missing = set(PHQ8Item.all_items()) - set(self.items.keys())
        if missing:
            raise ValueError(f"Missing PHQ-8 items: {missing}")

    @property
    def total_score(self) -> int:
        """Calculate total PHQ-8 score (0-24).

        N/A scores contribute 0 to the total.

        Returns:
            Sum of all item scores (0-24 range).
        """
        return sum(item.score_value for item in self.items.values())

    @property
    def severity(self) -> SeverityLevel:
        """Determine severity from total score.

        Returns:
            SeverityLevel based on paper thresholds.
        """
        return SeverityLevel.from_total_score(self.total_score)

    @property
    def available_count(self) -> int:
        """Count items with available (non-N/A) scores.

        Returns:
            Number of items with numeric scores.
        """
        return sum(1 for item in self.items.values() if item.is_available)

    @property
    def na_count(self) -> int:
        """Count items with N/A scores.

        Returns:
            Number of items without numeric scores.
        """
        return 8 - self.available_count

    def get_item(self, item: PHQ8Item) -> ItemAssessment:
        """Get assessment for specific item.

        Args:
            item: The PHQ8Item to retrieve.

        Returns:
            The ItemAssessment for the requested item.
        """
        return self.items[item]


@dataclass
class QualitativeAssessment:
    """Qualitative assessment output.

    Contains the narrative assessment generated by the qualitative
    assessment agent, organized into clinical domains.
    """

    overall: str
    """Overall summary of the patient's mental health state."""

    phq8_symptoms: str
    """Summary of PHQ-8 related symptoms and their frequencies."""

    social_factors: str
    """Social factors affecting mental health (relationships, support)."""

    biological_factors: str
    """Biological factors (family history, medical conditions)."""

    risk_factors: str
    """Risk factors identified (stressors, isolation, etc.)."""

    participant_id: int
    """Identifier of the assessed participant."""

    supporting_quotes: list[str] = field(default_factory=list)
    """Direct quotes from transcript supporting the assessment."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the assessment was created."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this assessment."""

    def __post_init__(self) -> None:
        """Validate participant_id is positive.

        Raises:
            ValueError: If participant_id is non-positive.
        """
        if self.participant_id <= 0:
            raise ValueError("Participant ID must be positive")

    @property
    def full_text(self) -> str:
        """Get full assessment as formatted text.

        Returns:
            Formatted string with all assessment sections.
        """
        return f"""Overall Assessment:
{self.overall}

PHQ-8 Symptoms:
{self.phq8_symptoms}

Social Factors:
{self.social_factors}

Biological Factors:
{self.biological_factors}

Risk Factors:
{self.risk_factors}
"""


@dataclass
class QualitativeEvaluation:
    """Evaluation of a qualitative assessment.

    Contains the judge agent's evaluation scores and explanations
    for the four quality metrics.
    """

    scores: Mapping[EvaluationMetric, EvaluationScore]
    """Scores for each evaluation metric."""

    assessment_id: UUID
    """ID of the QualitativeAssessment being evaluated."""

    iteration: int = 0
    """Feedback loop iteration number (0 = initial)."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the evaluation was created."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this evaluation."""

    def __post_init__(self) -> None:
        """Validate all 4 metrics are present.

        Raises:
            ValueError: If any evaluation metrics are missing.
        """
        missing = set(EvaluationMetric.all_metrics()) - set(self.scores.keys())
        if missing:
            raise ValueError(f"Missing evaluation metrics: {missing}")

    @property
    def average_score(self) -> float:
        """Calculate average score across all metrics.

        Returns:
            Mean of all metric scores (1.0-5.0 range).
        """
        return sum(s.score for s in self.scores.values()) / len(self.scores)

    @property
    def low_scores(self) -> list[EvaluationMetric]:
        """Get list of metrics with low scores (<=3).

        Returns:
            List of metrics that need improvement.
        """
        return [m for m, s in self.scores.items() if s.is_low]

    def low_scores_for_threshold(self, threshold: int) -> list[EvaluationMetric]:
        """Get list of metrics with scores at or below a threshold.

        Args:
            threshold: Score threshold (1-5).

        Returns:
            List of metrics that need improvement for the given threshold.
        """
        return [m for m, s in self.scores.items() if s.score <= threshold]

    @property
    def needs_improvement(self) -> bool:
        """Check if any metric needs improvement.

        Returns:
            True if any metric has a low score.
        """
        return len(self.low_scores) > 0

    @property
    def all_acceptable(self) -> bool:
        """Check if all metrics are acceptable (>=4).

        Returns:
            True if all scores are 4 or 5.
        """
        return all(s.is_acceptable for s in self.scores.values())

    def get_score(self, metric: EvaluationMetric) -> EvaluationScore:
        """Get score for specific metric.

        Args:
            metric: The EvaluationMetric to retrieve.

        Returns:
            The EvaluationScore for the requested metric.
        """
        return self.scores[metric]


@dataclass
class MetaReview:
    """Integrated meta-review combining all assessments.

    Contains the final integrated assessment from the meta-review
    agent, combining quantitative and qualitative findings.
    """

    severity: SeverityLevel
    """Final severity determination."""

    explanation: str
    """Explanation of how severity was determined."""

    quantitative_assessment_id: UUID
    """ID of the PHQ8Assessment used."""

    qualitative_assessment_id: UUID
    """ID of the QualitativeAssessment used."""

    participant_id: int
    """Identifier of the assessed participant."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the meta-review was created."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this meta-review."""

    @property
    def is_mdd(self) -> bool:
        """Check if indicates Major Depressive Disorder.

        Returns:
            True if severity indicates MDD (>=10 total score).
        """
        return self.severity.is_mdd


@dataclass
class FullAssessment:
    """Complete assessment result combining all components.

    Aggregates all assessment artifacts for a single participant
    into a single cohesive result.
    """

    transcript: Transcript
    """The source interview transcript."""

    quantitative: PHQ8Assessment
    """The quantitative (PHQ-8) assessment."""

    qualitative: QualitativeAssessment
    """The qualitative (narrative) assessment."""

    qualitative_evaluation: QualitativeEvaluation
    """The judge agent's evaluation of the qualitative assessment."""

    meta_review: MetaReview
    """The integrated meta-review."""

    created_at: datetime = field(default_factory=lambda: datetime.now(UTC))
    """Timestamp when the full assessment was compiled."""

    id: UUID = field(default_factory=uuid4)
    """Unique identifier for this full assessment."""

    @property
    def participant_id(self) -> int:
        """Get participant ID from transcript.

        Returns:
            The participant_id from the source transcript.
        """
        return self.transcript.participant_id

    @property
    def final_severity(self) -> SeverityLevel:
        """Get final severity from meta-review.

        Returns:
            The severity determination from the meta-review agent.
        """
        return self.meta_review.severity
